{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "AML Project Work.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daffe2b8"
      },
      "source": [
        "# AML Project Work 2021\n"
      ],
      "id": "daffe2b8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe141740"
      },
      "source": [
        "#Data Preprocessing\n",
        "\n",
        "link to datasets : https://www.kaggle.com/nicholasjhana/energy-consumption-generation-prices-and-weather\n"
      ],
      "id": "fe141740"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocessing of weather data"
      ],
      "metadata": {
        "id": "dw3U4M6bgiAI"
      },
      "id": "dw3U4M6bgiAI"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from warnings import filterwarnings\n",
        "filterwarnings('ignore')\n",
        "\n",
        "#Reading weather data\n",
        "Wdata = pd.read_csv('weather_features.csv', skipinitialspace=True) #Weather Data\n",
        "Wdata.info()\n",
        "\n",
        "#Dropping weather related columns with data of no interest\n",
        "Wdata=Wdata.drop(['dt_iso','temp_min', 'temp_max', 'wind_deg', 'weather_id',\n",
        "                  'weather_main', 'weather_description', 'weather_icon',\n",
        "                  'rain_3h', 'snow_3h'], axis=1)\n",
        "\n",
        "#Splitting the dataframe into 5 different dataframes by city name\n",
        "print(Wdata['city_name'].unique())\n",
        "grouped = Wdata.groupby(Wdata.city_name)\n",
        "\n",
        "Valencia = grouped.get_group(\"Valencia\") \n",
        "Madrid = grouped.get_group(\"Madrid\") \n",
        "Bilbao = grouped.get_group(\"Bilbao\") \n",
        "Barcelona = grouped.get_group(\"Barcelona\") \n",
        "Seville = grouped.get_group(\"Seville\") \n",
        "\n",
        "#Renaming columns in each dataframe according to cities\n",
        "\n",
        "Valencia = Valencia.rename(columns={'city_name': 'city_valencia', 'temp':'val_temp',\n",
        "                                    'pressure':'val_pressure', 'humidity':'val_humidity',\n",
        "                                    'wind_speed':'val_wind_speed','rain_1h':'val_rain_1h',\n",
        "                                    'clouds_all':'val_clouds_all'})\n",
        "\n",
        "Madrid = Madrid.rename(columns={'city_name': 'city_madrid', 'temp':'mad_temp',\n",
        "                                    'pressure':'mad_pressure', 'humidity':'mad_humidity',\n",
        "                                    'wind_speed':'mad_wind_speed','rain_1h':'mad_rain_1h',\n",
        "                                    'clouds_all':'mad_clouds_all'})\n",
        "\n",
        "Bilbao = Bilbao.rename(columns={'city_name': 'city_bilbao', 'temp':'bil_temp',\n",
        "                                    'pressure':'bil_pressure', 'humidity':'bil_humidity',\n",
        "                                    'wind_speed':'bil_wind_speed','rain_1h':'bil_rain_1h',\n",
        "                                    'clouds_all':'bil_clouds_all'})\n",
        "\n",
        "Barcelona = Barcelona.rename(columns={'city_name': 'city_barcelona', 'temp':'bar_temp',\n",
        "                                    'pressure':'bar_pressure', 'humidity':'bar_humidity',\n",
        "                                    'wind_speed':'bar_wind_speed','rain_1h':'bar_rain_1h',\n",
        "                                    'clouds_all':'bar_clouds_all'})\n",
        "\n",
        "Seville = Seville.rename(columns={'city_name': 'city_seville', 'temp':'sev_temp',\n",
        "                                    'pressure':'sev_pressure', 'humidity':'sev_humidity',\n",
        "                                    'wind_speed':'sev_wind_speed','rain_1h':'sev_rain_1h',\n",
        "                                    'clouds_all':'sev_clouds_all'})\n",
        "\n",
        "#Resetting indexes of the individual dataframes \n",
        "Madrid = Madrid.reset_index().drop(['index'], axis=1)\n",
        "Bilbao = Bilbao.reset_index().drop(['index'], axis=1)\n",
        "Barcelona = Barcelona.reset_index().drop(['index'], axis=1)\n",
        "Seville = Seville.reset_index().drop(['index'], axis=1)\n",
        "\n",
        "#Merging the five dataframes back into one dataframe\n",
        "merged_df = pd.concat([Valencia, Madrid, Bilbao, Barcelona, Seville], axis=1)\n",
        "\n",
        "#Dropping all rows containing NaN-values\n",
        "merged_df = merged_df.dropna()\n",
        "\n",
        "#Calculating the mean values of the columns containing same type of data\n",
        "#but from different cities\n",
        "\n",
        "#Mean temp\n",
        "merged_df['temp_mean'] = merged_df[['val_temp','mad_temp','bil_temp',\n",
        "                               'bar_temp','sev_temp']].mean(axis=1)\n",
        "#Mean air pressure\n",
        "merged_df['pressure_mean'] = merged_df[['val_pressure','mad_pressure','bil_pressure',\n",
        "                               'bar_pressure','sev_pressure']].mean(axis=1)\n",
        "#Mean humidity\n",
        "merged_df['humidity_mean'] = merged_df[['val_humidity','mad_humidity','bil_humidity',\n",
        "                               'bar_humidity','sev_humidity']].mean(axis=1)\n",
        "#Mean windspeed\n",
        "merged_df['wind_speed_mean'] = merged_df[['val_wind_speed','mad_wind_speed','bil_wind_speed',\n",
        "                               'bar_wind_speed','sev_wind_speed']].mean(axis=1)\n",
        "#Mean hourly rain\n",
        "merged_df['rain_1h_mean'] = merged_df[['val_rain_1h','mad_rain_1h','bil_rain_1h',\n",
        "                               'bar_rain_1h','sev_rain_1h']].mean(axis=1)\n",
        "#Mean cloudiness(%)\n",
        "merged_df['clouds_all_mean'] = merged_df[['val_clouds_all','mad_clouds_all','bil_clouds_all',\n",
        "                               'bar_clouds_all','sev_clouds_all']].mean(axis=1)\n",
        "\n",
        "#Dropping all the unnecessary columns now that we have mean values \n",
        "merged_df.drop(merged_df.columns.difference(['temp_mean','pressure_mean','humidity_mean',\n",
        "                                             'wind_speed_mean','rain_1h_mean',\n",
        "                                             'clouds_all_mean']), 1, inplace=True)\n",
        "\n",
        "#Storing the final result in an object \"Wdata_final\"                                          \n",
        "Wdata_final = merged_df\n",
        "Wdata_final.info()\n",
        "print(Wdata_final)"
      ],
      "metadata": {
        "id": "m9LswvNHghNa"
      },
      "id": "m9LswvNHghNa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocessing of energy data"
      ],
      "metadata": {
        "id": "QVbJBddjhyQT"
      },
      "id": "QVbJBddjhyQT"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25ac66c7"
      },
      "source": [
        "#Reading the data\n",
        "Edata = pd.read_csv('energy_dataset.csv') #Energy Data\n",
        "print(Edata.head())\n",
        "Edata.info()\n",
        "#Checking the datatype and number of unique values in the columns\n",
        "for i in Edata.columns:\n",
        "    print(i,\"--->\",Edata[i].nunique(),\"--->\",Edata[i].dtypes)\n",
        "\n",
        "#Checking the values of columns that have only one value\n",
        "\n",
        "cols=['generation fossil coal-derived gas','generation fossil oil shale','generation fossil peat',\n",
        "      'generation geothermal','generation marine','generation wind offshore']\n",
        "\n",
        "for values in cols:\n",
        "    print(Edata[values].unique())\n",
        "\n",
        "#Dropping the columns that don't have any other values than 0\n",
        "Edata=Edata.drop(['generation fossil coal-derived gas','generation fossil oil shale',\n",
        "                  'generation fossil peat','generation geothermal','generation marine',\n",
        "                  'generation wind offshore'],axis=1)\n",
        "\n",
        "#Dropping columns that have no values in them\n",
        "Edata=Edata.drop(['generation hydro pumped storage aggregated', \n",
        "                  'forecast wind offshore eday ahead'],axis=1)\n",
        "\n",
        "#Changing the datatype of the time column \n",
        "Edata[['Date','Time']]=Edata['time'].str.split(\" \",n=1,expand=True)\n",
        "Edata['Date']=pd.to_datetime(Edata['Date'])\n",
        "Edata[['Time','Spare']]=Edata['Time'].str.split(\"+\",n=1,expand=True)\n",
        "Edata=Edata.drop([\"Spare\",\"time\"],axis=1)\n",
        "Edata['Time']=pd.to_datetime(Edata['Time'],format='%H:%M:%S')\n",
        "Edata['Time']=Edata['Time'].dt.time\n",
        "\n",
        "#Moving the \"Date\" and \"Time\" columns to the front of the columns for a cleaner look\n",
        "cols = Edata.columns.tolist()\n",
        "cols = cols[-1:] + cols[:-1] #Moving 'Time' to the front of the columns\n",
        "cols = cols[-1:] + cols[:-1] #Moving 'Date' to the front of the columns\n",
        "Edata = Edata[cols] #Reordering the dataframe\n",
        "\n",
        "#Dropping unneccessary columns, for example not interested in energy price in this case\n",
        "Edata_final=Edata.drop(['price day ahead','price actual','total load forecast',\n",
        "                        'forecast solar day ahead','forecast wind onshore day ahead',\n",
        "                        'Date','Time'], axis=1)\n",
        "\n",
        "#Edata_final = Edata[['Date','Time','total load actual']]\n",
        "Edata_final.info()\n",
        "print(Edata_final)"
      ],
      "id": "25ac66c7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "YTHP_yr_kiht"
      },
      "id": "YTHP_yr_kiht",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Combining the two dataframes and finalizing the dataframe"
      ],
      "metadata": {
        "id": "lCfwUJECKwca"
      },
      "id": "lCfwUJECKwca"
    },
    {
      "cell_type": "code",
      "source": [
        "#Combining weather and energy dataframes into one dataframe\n",
        "final_dframe = pd.concat([Edata_final,Wdata_final], axis=1)\n",
        "\n",
        "#Dropping all rows containing NaN-values\n",
        "final_dframe = final_dframe.dropna()\n",
        "\n",
        "final_dframe.info()\n",
        "print(final_dframe)"
      ],
      "metadata": {
        "id": "tG3vRFIEKxGZ"
      },
      "id": "tG3vRFIEKxGZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Further data analysis"
      ],
      "metadata": {
        "id": "IQ8EdplILdVM"
      },
      "id": "IQ8EdplILdVM"
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking for outliers in the weather data columns\n",
        "sns.boxplot(final_dframe['temp_mean'])\n",
        "plt.show()\n",
        "\n",
        "sns.boxplot(final_dframe['pressure_mean'])\n",
        "plt.show()\n",
        "\n",
        "sns.boxplot(final_dframe['humidity_mean'])\n",
        "plt.show()\n",
        "\n",
        "sns.boxplot(final_dframe['wind_speed_mean'])\n",
        "plt.show()\n",
        "\n",
        "sns.boxplot(final_dframe['rain_1h_mean'])\n",
        "plt.show()\n",
        "\n",
        "sns.boxplot(final_dframe['clouds_all_mean'])\n",
        "plt.show()\n",
        "\n",
        "#Fixing the outliers in all columns, except 'temp_mean'\n",
        "from scipy.stats.mstats import winsorize\n",
        "final_dframe['pressure_mean']=winsorize(final_dframe['pressure_mean'],(0.1,0.05))\n",
        "final_dframe['humidity_mean']=winsorize(final_dframe['humidity_mean'],(0.05,0.05))\n",
        "final_dframe['wind_speed_mean']=winsorize(final_dframe['wind_speed_mean'],(0,0.05))\n",
        "final_dframe['rain_1h_mean']=winsorize(final_dframe['rain_1h_mean'],(0,0.2))\n",
        "final_dframe['clouds_all_mean']=winsorize(final_dframe['clouds_all_mean'],(0,0.025))\n",
        "\n",
        "sns.boxplot(final_dframe['pressure_mean'])\n",
        "plt.show()\n",
        "sns.boxplot(final_dframe['humidity_mean'])\n",
        "plt.show()\n",
        "sns.boxplot(final_dframe['wind_speed_mean'])\n",
        "plt.show()\n",
        "sns.boxplot(final_dframe['rain_1h_mean'])\n",
        "plt.show()\n",
        "sns.boxplot(final_dframe['clouds_all_mean'])\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#Checking if there still are some missing values\n",
        "final_dframe.isnull().sum()\n",
        "\n"
      ],
      "metadata": {
        "id": "rNETHBVPiH-K"
      },
      "id": "rNETHBVPiH-K",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Correlation between variables"
      ],
      "metadata": {
        "id": "3nFDGlGop70Z"
      },
      "id": "3nFDGlGop70Z"
    },
    {
      "cell_type": "code",
      "source": [
        "#Findig the correlation between variables\n",
        "final_dframe.corr()\n",
        "\n",
        "#plotting the correlation between variables in which correlation is high\n",
        "df5=final_dframe.corr()\n",
        "plt.figure(figsize=(15, 10))\n",
        "sns.heatmap(df5[(df5>0.5)|(df5<-0.5)],annot=True,cbar=False,linewidth=0.5,linecolor='blue')"
      ],
      "metadata": {
        "id": "2frRv0qPp_-8"
      },
      "id": "2frRv0qPp_-8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Final data preparation for model building"
      ],
      "metadata": {
        "id": "itMWeIiw6S7V"
      },
      "id": "itMWeIiw6S7V"
    },
    {
      "cell_type": "code",
      "source": [
        "dframe_reg = final_dframe\n",
        "\n",
        "\n",
        "X=dframe_reg.drop('total load actual',axis=1)\n",
        "y=dframe_reg['total load actual']\n",
        "\n",
        "#Splitting the dataframe X and y into train and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 1)\n",
        "\n",
        "#checking the dimensions of the train & test subset\n",
        "print(\"The shape of X_train is:\",X_train.shape)\n",
        "print(\"The shape of X_test is:\",X_test.shape)\n",
        "print(\"The shape of y_train is:\",y_train.shape)\n",
        "print(\"The shape of y_test is:\",y_test.shape)"
      ],
      "metadata": {
        "id": "b390DYoo6Xo-"
      },
      "id": "b390DYoo6Xo-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8f663ae"
      },
      "source": [
        "# Model Construction and Validation\n"
      ],
      "id": "d8f663ae"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ExtraTreesRegressor"
      ],
      "metadata": {
        "id": "VfcOstGq22w8"
      },
      "id": "VfcOstGq22w8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6766a2fb"
      },
      "source": [
        "#Building the ExtraTreesRegressor model\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "etr=ExtraTreesRegressor(n_estimators=25, max_depth=10).fit(X_train, y_train)\n",
        "\n",
        "# Set the parameters by cross-validation\n",
        "tuned_parameters = [{'max_depth': range(5,15), \n",
        "                     'n_estimators': range(20,30)}]\n",
        "\n",
        "# Use the GridSearch to find out the best paramers using 5 fold cross validation\n",
        "tune_et=GridSearchCV(etr, tuned_parameters, cv=5);\n",
        "tune_et.fit(X_train, y_train);\n",
        "optimal_et=tune_et.best_estimator_\n",
        "\n",
        "#Training the regressor\n",
        "train_pred=etr.predict(X_train)\n",
        "\n",
        "#Using cross validation score to test the performance of the regressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "cv_score=cross_val_score(etr, X_train, y_train, cv=5).mean()\n",
        "print(\"Accurary in crossvalidation...%f\" % cv_score)\n",
        "\n",
        "#Testing the regressor\n",
        "test_pred=etr.predict(X_test)\n",
        "\n",
        "#Reporting the train and test score of the regressor\n",
        "from sklearn.metrics import r2_score\n",
        "train_score = r2_score(y_train, train_pred)\n",
        "test_score = r2_score(y_test, test_pred)\n",
        "print(\"Accurary in the training set..%f\" % train_score)\n",
        "print(\"Accurary in the test set......%f\" % test_score)\n",
        "print(optimal_et)"
      ],
      "id": "6766a2fb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Vizualization of ExtraTreesRegressor results"
      ],
      "metadata": {
        "id": "qwlY40MY9oJL"
      },
      "id": "qwlY40MY9oJL"
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting\n",
        "import seaborn as sns\n",
        "sns.regplot(x=y_test, y=test_pred, line_kws={\"color\": \"red\"})\n",
        "plt.xlabel('Actual Energy Demand')\n",
        "plt.ylabel('Predicted Energy Demand')\n",
        "print()"
      ],
      "metadata": {
        "id": "EuYvc8ZA9r1a"
      },
      "id": "EuYvc8ZA9r1a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##GradientBoostingRegressor "
      ],
      "metadata": {
        "id": "YbsxqQjfYs1Q"
      },
      "id": "YbsxqQjfYs1Q"
    },
    {
      "cell_type": "code",
      "source": [
        "#Building the GradientBoostingRegressor model\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "gbr = GradientBoostingRegressor(n_estimators=25, max_depth=10, loss='squared_error').fit(X_train, y_train)\n",
        "\n",
        "# Set the parameters by cross-validation\n",
        "tuned_parameters = [{'max_depth': range(5,15), \n",
        "                     'n_estimators': range(20,30)}]\n",
        "\n",
        "# Use the GridSearch to find out the best paramers using 5 fold cross validation\n",
        "tune_et=GridSearchCV(etr, tuned_parameters, cv=3);\n",
        "tune_et.fit(X_train, y_train);\n",
        "optimal_et=tune_et.best_estimator_\n",
        "\n",
        "#Training the regressor\n",
        "train_pred=gbr.predict(X_train)\n",
        "\n",
        "#Using cross validation score to test the performance of the regressor\n",
        "cv_score=cross_val_score(gbr, X_train, y_train, cv=5).mean()\n",
        "print(\"Accurary in crossvalidation...%f\" % cv_score)\n",
        "\n",
        "#Testing the regressor\n",
        "test_pred=gbr.predict(X_test)\n",
        "\n",
        "#Reporting the train and test score of the regressor\n",
        "train_score = r2_score(y_train, train_pred)\n",
        "test_score = r2_score(y_test, test_pred)\n",
        "print(\"Accurary in the training set..%f\" % train_score)\n",
        "print(\"Accurary in the test set......%f\" % test_score)\n",
        "print(optimal_et)"
      ],
      "metadata": {
        "id": "muFrbKwTYsSF"
      },
      "id": "muFrbKwTYsSF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Vizualization of GradientBoostingRegressor results"
      ],
      "metadata": {
        "id": "bbOitXjh4OZY"
      },
      "id": "bbOitXjh4OZY"
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting\n",
        "sns.regplot(x=y_test, y=test_pred, line_kws={\"color\": \"red\"})\n",
        "plt.xlabel('Actual Energy Demand')\n",
        "plt.ylabel('Predicted Energy Demand')\n",
        "print()"
      ],
      "metadata": {
        "id": "OL7FNGKJ4RuK"
      },
      "id": "OL7FNGKJ4RuK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the parameters by cross-validation\n",
        "tuned_parameters = [{'max_depth': range(10,20), 'min_samples_split': range(2,5),\n",
        "                     'n_estimators': range(30,50)}]\n",
        "\n",
        "# Use the GridSearch to find out the best paramers using 5 fold cross validation\n",
        "tune_et=GridSearchCV(etr, tuned_parameters, cv=3);\n",
        "tune_et.fit(X_train, y_train);\n",
        "optimal_et=tune_et.best_estimator_"
      ],
      "metadata": {
        "id": "JkAigCQUGtlc"
      },
      "id": "JkAigCQUGtlc",
      "execution_count": null,
      "outputs": []
    }
  ]
}